{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":30673,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Movie Sentiment Analysis: Review Polarity Classification\n\n## Background\n\nThis project focuses on sentiment analysis of movie reviews, aimed at determining the underlying sentiment expressed within a body of text. By analyzing the content of movie reviews, we strive to classify each review as positive or negative automatically. \n\nDataset can be found in [here](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews).\n\n\n## **Plan**\n\n### 1.1 Plan\n\nWe need to perform data cleaning and use Multilayer Perceptrons (MLP) to build a model, and we will use the model we build to predict our text.\n\n## **Analyze**\n\n### 2.1 Import, Load and Examine ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\nfrom bs4 import BeautifulSoup\nimport string\nfrom collections import Counter\nfrom wordcloud import WordCloud\nfrom scipy.stats import randint\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"IMDB_Dataset.csv\")\ndf","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2 Data Cleaning\n\n- **HTML tags:** If dataset contain HTML tags that we will need to remove it.\n- **Special characters and numbers:** Special characters and numbers do not contribute to sentiment analysis and can be removed.\n- **Punctuation:** Punctuation marks can be removed, depends on different approach.\n- **Lowercase** all sentences to make sure they look same in our model.\n- **Missing data** Missing data row should be delete.\n\n#### 2.2.1 Remove HTML Tags\n","metadata":{}},{"cell_type":"code","source":"# Remove HTML tags\ndef remove_html_tags(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    return soup.get_text()\n\n# Apply function to dataset\ndf[\"review\"] = df[\"review\"].apply(remove_html_tags)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.2.2 Remove Special Characters & Numbers","metadata":{}},{"cell_type":"code","source":"# Remove pecial characters and numbers\ndf[\"review\"] = df[\"review\"].str.replace(\"[^a-zA-Z]\", \" \", regex=True)\n\n# Remove extra spaces\ndf[\"review\"] = df[\"review\"].str.replace(\"\\s+\", \" \", regex=True).str.strip()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.2.3 Remove Punctuation","metadata":{}},{"cell_type":"code","source":"# Remove punctuation\npunctuation_pattern = f\"[{string.punctuation}]\"\ndf[\"review\"] = df[\"review\"].str.replace(punctuation_pattern, \"\", regex=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.2.4 Lowercase","metadata":{}},{"cell_type":"code","source":"# Lowercase\ndf[\"review\"] = df[\"review\"].str.lower()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.2.5 Missing Value","metadata":{}},{"cell_type":"code","source":"df.isna().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3 Data Processing\n\n#### 2.3.1 Tokenization","metadata":{}},{"cell_type":"code","source":"# Tokenization\ndf[\"review\"] = df[\"review\"].apply(word_tokenize)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.3.2 Remove Stop Words","metadata":{}},{"cell_type":"code","source":"# Remove stop words\nnltk.download(\"stopwords\")\nstop_words = set(stopwords.words(\"english\"))\n\ndef remove_stop_words(token_list):\n    return [word for word in token_list if word not in stop_words]\n\ndf[\"review\"] = df[\"review\"].apply(remove_stop_words)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.3.3 Lemmatization / Stemming\n\nChoosing Between Lemmatization and Stemming\n- **Lemmatization** is generally more sophisticated and produces more grammatically correct results (actual words), but it's computationally more intensive.\n- **Stemming** is faster but may produce results that are not actual words (e.g., \"runn\" for \"running\").\n\nThese processes help in reducing the number of unique tokens in the model needs to learn about.","metadata":{}},{"cell_type":"code","source":"# Stemming\ndef stem_tokens(token_list):\n    stemmer = PorterStemmer()\n    return [stemmer.stem(token) for token in token_list]\n\ndf[\"review\"] = df[\"review\"].apply(stem_tokens)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3 Extract High-Frequency Words","metadata":{}},{"cell_type":"code","source":"# Extract high-frequency words\nall_tokens = [token for sublist in df[\"review\"] for token in sublist]\nword_freq = Counter(all_tokens)\nmost_common_words = word_freq.most_common(20)\nmost_common_words","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize it using wordcloud\nwordcloud = WordCloud(width=800, height=800, background_color='white').generate_from_frequencies(word_freq)\n\nplt.figure(figsize=(15, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")  # Hide the axes\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.4 Text Vectorization\n\n**TF-IDF**","metadata":{}},{"cell_type":"code","source":"# Join the tokens back into strings for the vectorizer\ndf[\"joined_review\"] = df[\"review\"].apply(\" \".join)\ntext_data = df[\"joined_review\"]\ntfidf_vectorizer = TfidfVectorizer(max_features=2000)\n\n# Fit the vectorizer to the text data and transform it into TF-IDF features\nx_tfidf = tfidf_vectorizer.fit_transform(text_data)\nx_tfidf\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Construct**\n\nWe will build model use **Multilayer Perceptron (MLP)**.\n\n### 3.1 Split Dataset","metadata":{}},{"cell_type":"code","source":"# Converting categorical labels to numerical form\n\ndf[\"sentiment_numeric\"] = df[\"sentiment\"].map({\"positive\": 1, \"negative\": 0})\n\n# Split dataset\nx_train, x_test, y_train, y_test = train_test_split(x_tfidf, df[\"sentiment_numeric\"], test_size=0.2, random_state=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2 Model Initialization and Training","metadata":{}},{"cell_type":"code","source":"# Scale data\nscaler = MaxAbsScaler()\n\n# Scale the training and test sets\nx_train_scaled = scaler.fit_transform(x_train)\nx_test_scaled = scaler.transform(x_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train MLP model\nmlp_model = MLPClassifier()\nmlp_model.fit(x_train_scaled, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the Model\ny_pred = mlp_model.predict(x_test_scaled)\n\n# Calculate accuracy\nmlp_accuracy = accuracy_score(y_test, y_pred)\nprint(f\"Model accuracy: {mlp_accuracy}\")\n\n# Print classification report for a detailed performance analysis\nprint(classification_report(y_test, y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Due to the extensive computational resources and time required for hyperparameter tuning, I am unable to proceed with this step. Nonetheless, using the default parameters, we have achieved an accuracy of 87%, and we will consider this satisfactory for our current purposes.\n\n### 3.3 Evaluate Model\n\n#### 3.3.1 Confusion Matrix","metadata":{}},{"cell_type":"code","source":"# Confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"])\nplt.title(\"Confusion Matrix\")\nplt.ylabel(\"Actual Label\")\nplt.xlabel(\"Predicted Label\")\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.3.2 Scores","metadata":{}},{"cell_type":"code","source":"# Define a function to get all scores\ndef get_scores(y_true, y_pred_prob, threshold=0.5):\n\n    y_pred = (y_pred_prob[:, 1] >= threshold).astype(int)\n    \n    scores = {\n        'Accuracy': accuracy_score(y_true, y_pred),\n        'Precision': precision_score(y_true, y_pred),\n        'Recall': recall_score(y_true, y_pred),\n        'F1': f1_score(y_true, y_pred),\n        'Roc_Auc': roc_auc_score(y_true, y_pred_prob[:, 1])\n    }\n    \n    return scores\n\ny_pred_mlp_prob = mlp_model.predict_proba(x_test_scaled)\n\n# Now call the function with the true labels and the predicted probabilities\nscores_mlp = get_scores(y_test, y_pred_mlp_prob)\nscores_mlp\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Execute**\n\n### 4.1 Make New Prediction","metadata":{}},{"cell_type":"code","source":"# Sample text to predict sentiment\nsample_text = [\n    \"This movie was a great watch with brilliant performances and a gripping plot!\",  # Positive\n    \"An absolute waste of time, the worst movie I've seen in a long while.\",  # Negative\n    \"I found the movie to be mediocre, not terrible but not great either.\",  # Neutral\n    \"The cinematography was stunning, but the storyline was lacking and unoriginal.\",  # Neutral/Negative\n    \"The film was a masterpiece with a perfect blend of drama and action, a must-watch!\",  # Positive\n    \"It was an okay movie; I neither liked it nor disliked it particularly.\",  # Neutral\n    \"The plot twist at the end was predictable and uninspired.\",  # Negative\n    \"A stellar cast, but the film fell flat due to poor writing.\",  # Negative\n    \"I loved the special effects, but the characters were not very compelling.\",  # Neutral/Negative\n    \"The movie was well-received by critics but I didn't find it very interesting.\",  # Neutral\n    \"This film is overrated, I had high expectations but was sadly disappointed.\",  # Negative\n    \"What an entertaining experience, I was on the edge of my seat the whole time!\"  # Positive\n]\n\nfor text in sample_text:\n    # Convert the text to TF-IDF features\n    sample_tfidf = tfidf_vectorizer.transform([text])\n\n    # Scale the features because we scaled during training\n    sample_scaled = scaler.transform(sample_tfidf)\n\n    # Make a prediction\n    prediction_prob = mlp_model.predict_proba(sample_scaled)\n\n    # Get the predicted probability of the positive class\n    positive_prob = prediction_prob[0, 1]\n\n    # Apply threshold to convert probabilities to class labels\n    prediction = 1 if positive_prob > 0.5 else 0\n\n    # Output the prediction\n    sentiment = \"Positive\" if prediction == 1 else \"Negative\"\n    print(f\"{text} ->  {sentiment}\")\n    # print(f\"Predicted sentiment: {sentiment}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2 Result & Conclusion\n\nOur model achieved the following performance metrics:\n\n- Accuracy: 0.8782\n- Precision: 0.8683\n- Recall: 0.8890\n- F1 Score: 0.8786\n- ROC AUC: 0.9477\n\nThese results indicate a strong overall performance. Given the constraints on GPU resources, we were unable to conduct hyperparameter tuning. However, it's plausible that with further optimization, we could see an improvement in these scores.","metadata":{}}]}