{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. Install NLTK and download necessary resources:**"
      ],
      "metadata": {
        "id": "blQbyeGKhRF1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIlF3syYhPG4",
        "outputId": "ed35f3e2-ee46-403e-ec7e-5fd909dcc2fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')  # For sentence segmentation\n",
        "nltk.download('averaged_perceptron_tagger')  # For part-of-speech tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Prepare your text:**"
      ],
      "metadata": {
        "id": "NlbW17fWhaav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This is a sample text to demonstrate different tokenization techniques in NLTK. It includes punctuation, numbers, and abbreviations.\""
      ],
      "metadata": {
        "id": "fnPpszBGhYib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spliting"
      ],
      "metadata": {
        "id": "4HJECnzyfQzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the sentence using the split() method:\n",
        "split_tokens = text.split()\n",
        "print(\"Split Tokens:\", split_tokens)"
      ],
      "metadata": {
        "id": "HeU7Qix8fU-L",
        "outputId": "5e700a77-ebfe-4223-a023-877b15d8ed0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split Tokens: ['This', 'is', 'a', 'sample', 'text', 'to', 'demonstrate', 'different', 'tokenization', 'techniques', 'in', 'NLTK.', 'It', 'includes', 'punctuation,', 'numbers,', 'and', 'abbreviations.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Apply different tokenization techniques:**"
      ],
      "metadata": {
        "id": "UpGVSj8Shrax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a. Word Tokenization:"
      ],
      "metadata": {
        "id": "uwPjkiFhh2ZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "word_tokens = word_tokenize(text)\n",
        "print(\"Word Tokens:\", word_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyhoLe4chkSR",
        "outputId": "4b72a3c5-647e-4d36-f5f3-84324432a8ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokens: ['This', 'is', 'a', 'sample', 'text', 'to', 'demonstrate', 'different', 'tokenization', 'techniques', 'in', 'NLTK', '.', 'It', 'includes', 'punctuation', ',', 'numbers', ',', 'and', 'abbreviations', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. Sentence Tokenization:"
      ],
      "metadata": {
        "id": "SLdjiwMNh-MF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sentence_tokens = sent_tokenize(text)\n",
        "print(\"Sentence Tokens:\", sentence_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbFr05Ejh7KG",
        "outputId": "0aa5d237-4bf2-466d-f50c-7913abdd9571"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokens: ['This is a sample text to demonstrate different tokenization techniques in NLTK.', 'It includes punctuation, numbers, and abbreviations.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "c. TreebankWordTokenizer (handles contractions and hyphens):"
      ],
      "metadata": {
        "id": "sawHS_ojiCbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "treebank_tokenizer = TreebankWordTokenizer()\n",
        "treebank_tokens = treebank_tokenizer.tokenize(text)\n",
        "print(\"Treebank Tokens:\", treebank_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToMWrjTUiAdv",
        "outputId": "f7beea26-b736-447f-fe02-2e2cb7e676bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treebank Tokens: ['This', 'is', 'a', 'sample', 'text', 'to', 'demonstrate', 'different', 'tokenization', 'techniques', 'in', 'NLTK.', 'It', 'includes', 'punctuation', ',', 'numbers', ',', 'and', 'abbreviations', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "d. RegexpTokenizer (customizable with regular expressions):"
      ],
      "metadata": {
        "id": "wfS4JOzmiIhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "regexp_tokenizer = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S+')  # Split on words, dollar amounts, or any non-whitespace characters\n",
        "regexp_tokens = regexp_tokenizer.tokenize(text)\n",
        "print(\"Regexp Tokens:\", regexp_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1_8CXD0iEcD",
        "outputId": "dd13dd8d-cc81-48e4-c341-61cb013e581b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regexp Tokens: ['This', 'is', 'a', 'sample', 'text', 'to', 'demonstrate', 'different', 'tokenization', 'techniques', 'in', 'NLTK', '.', 'It', 'includes', 'punctuation', ',', 'numbers', ',', 'and', 'abbreviations', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "e. TweetTokenizer (optimized for tweets):"
      ],
      "metadata": {
        "id": "tXmdWmE9iNDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "tweet_tokens = tweet_tokenizer.tokenize(text)\n",
        "print(\"Tweet Tokens:\", tweet_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Sq3wMWNiKq4",
        "outputId": "5064ad1d-9ce9-4813-bebf-5228829487aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet Tokens: ['This', 'is', 'a', 'sample', 'text', 'to', 'demonstrate', 'different', 'tokenization', 'techniques', 'in', 'NLTK', '.', 'It', 'includes', 'punctuation', ',', 'numbers', ',', 'and', 'abbreviations', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QoKYTRP3fbAJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}